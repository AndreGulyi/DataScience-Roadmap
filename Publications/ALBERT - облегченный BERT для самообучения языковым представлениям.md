С тех пор как в 2018 году был представлен [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), исследования в области обработки естественного языка охвачены новой парадигмой: использованием больших объемов существующего текста для предварительного обучения параметров модели на основе самообучения (self-supervision), не требующего разметки данных. Таким образом, вместо того, чтобы обучать модель для [обработки естественного языка](https://en.wikipedia.org/wiki/Natural_language_processing) (NLP) с нуля, можно взять предобученную модель, уже имеющую некоторое знание о языке. Однако, для успешного применения этого нового подхода в NLP исследователю необходимо иметь некоторое представление о том, что же именно способствует языковому обучению модели: высота нейронной сети (т.е. количество слоев), ее ширина (размер представлений скрытых слоев), критерий обученности для самообучения или что-то совсем иное?

В статье «[ALBERT: облегченный BERT для самообучения языковым представлениям](https://arxiv.org/abs/1909.11942)», принятой на [ICLR 2020](https://iclr.cc/Conferences/2020), была представлена обновленная версия BERT’а, которая показывает более высокие результаты в 12 задачах обработки языка, включая соревнование [Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/) (SQuAD v2.0) и бенчмарк [RACE](http://www.qizhexie.com/data/RACE_leaderboard.html) для понимания текстов из экзаменов [SAT](https://en.wikipedia.org/wiki/SAT). ALBERT [выпускается](https://github.com/google-research/ALBERT) в качестве открытого решения поверх [TensorFlow](https://www.tensorflow.org/) и включает в себя несколько готовых к использованию предобученных языковых моделей.

# Что повышает качество NLP моделей?

Выделить главный фактор улучшения качества в NLP сложно: да, одни настройки важнее других, но, как показывает данное исследование, простой перебор этих настроек независимо друг от друга не даёт правильных ответов.

Ключом к оптимизации модели, заложенным в архитектуру ALBERT, является более эффективное распределение ее ресурсов. Входные эмбеддинги (слова, суб-токены и т.д.) выучивают векторные представления, независимые от контекста: например, представление слова «bank». Эмбеддинги скрытых слоев, напротив, должны уточнять значения этих векторных представлений в зависимости от конкретного контекста: например, вектор «bank» как финансового учреждения или как берега реки.

Это достигается путем факторизации параметризации эмбеддингов: матрица эмбеддингов разделяется между векторами входного слоя с относительно небольшой размерностью (например, 128), в то время как вектора скрытого слоя используют бОльшие размерности (768, как в случае с BERT'ом, и больше). Только с помощью этого шага, при прочих равных, ALBERT на 80% снижает количество параметров проекционного блока ценой лишь незначительного падения производительности – 80.3 балла вместо 80.4 для [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) и 67.9 вместо 68.2 для [RACE](http://www.qizhexie.com/data/RACE_leaderboard.html).

Другое важное изменение в архитектуре ALBERT связано с исследованием избыточности. Архитектуры нейронных сетей на основе [Трансформера](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html) (такие как [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [XLNet](https://www.borealisai.com/en/blog/understanding-xlnet/) и [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)) полагаются на независимость слоев, расположенных друг над другом. Однако было замечено, что зачастую нейросеть выучивается выполнять схожие операции на разных уровнях, используя различные параметры сети. Эта возможная избыточность устраняется в ALBERT с помощью обмена параметрами между слоями, и, таким образом, один и тот же слой применяется друг к другу. И хотя такой подход немного снижает точность (accuracy), более компактный размер самой модели оправдывает потерю в качестве. Подобный обмен обеспечивает снижение параметров для блока с [механизмом внимания](https://arxiv.org/abs/1706.03762) внимания на 90% (общее снижение на 70%), что при применении в дополнение к факторизации параметризации эмбеддингов приводит к небольшому снижению показателей: до 80.0 (-0.3) для [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) и до 64.0 (-3.9 балла) для [RACE](http://www.qizhexie.com/data/RACE_leaderboard.html).

Внедрение двух представленных изменений рождает модель ALBERT-base, которая имеет всего 12 миллионов параметров, что на 89% меньше базовой модели BERT, но при этом обеспечивает достойное качество в рассмотренных бенчмарках. Вместе с тем подобное уменьшение количества параметров дает возможность и дальнейшего масштабирования. Если объем памяти позволяет, можно увеличить размер эмбеддингов скрытого слоя в 10-20 раз. При размере скрытого слоя 4096 конфигурация ALBERT-xxlarge обеспечивает как общее снижение параметров на 30% по сравнению с моделью BERT-large, так и, что более важно, значительный прирост качества: +4.2 для [SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) (88.1 по сравнению с 83.9) и +8.5 для [RACE](http://www.qizhexie.com/data/RACE_leaderboard.html) (82.3 по сравнению с 73.8).

Полученные результаты свидетельствуют о том, что понимание языка зависит от разработки надежных и высокоёмких контекстных представлений. Контекст, смоделированный в эмбеддингах скрытого слоя, улавливает значение слов, что, в свою очередь, способствует общему пониманию, которое напрямую измеряется показателями модели в стандартных бенчмарках.

# Показатели оптимизированной модели на наборе данных RACE

Чтобы оценить способность модели понимать язык, можно провести тест на понимание прочитанного (например, схожий с [SAT Reading Test](https://collegereadiness.collegeboard.org/sat/inside-the-test/reading)). Крупнейший общедоступный ресурс для этой цели - [набор данных RACE](https://www.aclweb.org/anthology/D17-1082/) (2017 г.). То, как компьютер справляется с этим испытанием, хорошо отражает достижения в области языкового моделирования последних лет: модель, предварительно обученная только на контекстно-независимых представлениях слов, получает низкие баллы в этом тесте (45.9; крайний левый столбик), в то время как BERT, получивший контекстно-зависимое знание языка, справляется достаточно хорошо - 72.0 балла. Усовершенствованные модели BERT, такие как [XLNet](https://www.borealisai.com/en/blog/understanding-xlnet/) и [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/), установили планку еще выше - в диапазоне 82–83 баллов. Конфигурация ALBERT-xxlarge, упомянутая выше, дает оценку RACE в том же диапазоне (82.3) при обучении на наборе данных базового BERT'a (Википедия и Книги). Однако при обучении на том же расширенном наборе данных, что и XLNet и RoBERTa, она значительно превосходит все существующие на сегодняшний день подходы и устанавливает новую оценку в 89.4.

![image1](https://habrastorage.org/webt/v_/2f/iw/v_2fiwoatyh8hqvvjf2twox5lbw.png)

*Показатели моделей в бенчмарке RACE (задача на понимание текстов из экзаменов SAT). Оценка для случайного предсказания составляет 25.0. Максимально возможный балл – 95.0.*

Успех ALBERT демонстрирует важность выявления тех аспектов модели, которые помогают создать мощные контекстные представления. Сосредоточив свои усилия по улучшению на этих аспектах архитектуры, можно значительно повысить как эффективность модели, так и ее показатели в самых различных задачах обработки естественного языка. Для того, чтобы способствовать дальнейшему развитию в области NLP, авторы [открыли исходный код ALBERT](https://github.com/google-research/ALBERT) и приглашают сообщество исследователей к сотрудничеству.

# Авторы

* **Авторы оригинала** - Radu Soricut, Zhenzhong Lan
* **Перевод** - [Смирнова Екатерина](https://habr.com/ru/users/smekur/)
* **Редактирование и вёрстка** - [Шкарин Сергей](https://habr.com/ru/users/kouki_rus/)