Технологический мир [охватил](https://www.theverge.com/21346343/gpt-3-explainer-openai-examples-errors-agi-potential) новый хайп - GPT-3.

Огромные языковые модели (вроде GPT-3) все больше удивляют нас своими возможностями. И хотя пока доверие к ним со стороны бизнеса недостаточно для того, чтобы представить их своим клиентам, эти модели демонстрируют те зачатки разума, которые позволят ускорить развитие автоматизации и возможностей «умных» компьютерных систем. Давайте снимем ауру таинственности с GPT-3 и узнаем, как она обучается и как работает.

Обученная языковая модель генерирует текст. Мы можем также отправить на вход модели какой-то текст и посмотреть, как изменится выход. Последний генерируется из того, что модель «выучила» во время периода обучения путем анализа больших объемов текста.

![](https://habrastorage.org/webt/8o/c7/g9/8oc7g9innn4_n6mg0ryidnqzz7e.gif)

<cut/>

Обучение – это процесс передачи модели большого количества текста. Для GPT-3 этот процесс завершен и все эксперименты, которые вы сможете увидеть, проводятся на уже обученной модели. Было подсчитано, что обучение должно было занять 355 GPU-лет (355 лет обучения на одной видеокарте) и стоить 4.6 миллиона долларов.

![02-gpt3-training-language-model
](https://habrastorage.org/webt/ge/58/kl/ge58kl7civvhb2sumz49tazraii.gif)

Для генерации примеров для обучения модели был использован набор данных размером в 300 миллиардов текстовых токенов. Например, так выглядят три обучающих примера, полученных из одного предложения, изображенного сверху.

На изображении видно, как мы можем получить множество примеров, просто проходя окном по имеющемуся тексту.

![gpt3-training-examples-sliding-window](https://habrastorage.org/webt/l4/7y/9s/l47y9sn_uwpsrjxdr36_m5ww9_a.png)

На ввод модели мы подаем один пример (отображаем только признаки) и просим ее предсказать следующее слово предложения.

Поначалу предсказания модели будут ошибочны. Мы подсчитываем ошибку в предсказании и обновляем модель до тех пор, пока предсказания не улучшатся.

И так несколько миллионов раз.

![03-gpt3-training-step-back-prop](https://habrastorage.org/webt/32/w2/bu/32w2bu4fmycoja-kapw6juep9oa.gif)

Теперь давайте рассмотрим эти этапы обучения чуть более подробно.

GPT-3 генерирует выход по одному токену за раз (условимся пока, что токен – это одно слово).

![04-gpt3-generate-tokens-output](http://jalammar.github.io/images/gpt3/04-gpt3-generate-tokens-output.gif)

Стоит отметить, что эта статья - лишь описание работы GPT-3, а не обсуждение того, что нового эта модель предложила миру (по сути все сводится к до смешного огромным размерам). В основе архитектуры – модель декодирующего Трансформера, описанная в [статье](https://arxiv.org/pdf/1801.10198.pdf).

GPT-3 поистине ОГРОМНА. Она кодирует то, чему выучивается, в 175 миллиардов чисел (называемых параметрами). Эти числа используются для подсчета генерируемого за один прогон токена.

Необученная модель инициализирует параметры случайным образом, а затем в ходе обучения подбирает такие значения, которые помогут получить наилучшие предсказания.

![gpt3-parameters-weights](https://habrastorage.org/webt/ub/ta/5r/ubta5rjvaeuqjy0sohqit7unxjw.png)

Эти значения – часть сотен матриц внутри модели, а предсказания – главным образом, результат множества матричных перемножений.

В видео [«Введение в ИИ на Youtube»](https://arxiv.org/pdf/1801.10198.pdf) показана простая модель машинного обучения с одним параметром – отличное начало для разбора этого 175-миллиардного монстра.

Чтобы пролить свет на то, как эти параметры распределяются и используются, нам нужно открыть модель и посмотреть на нее изнутри.

Ширина GPT-3 составляет 2048 токенов – это её «контекстное окно», что означает наличие 2048 траекторий, по которым продвигаются токены во время их обработки.

![05-gpt3-generate-output-context-window](http://jalammar.github.io/images/gpt3/05-gpt3-generate-output-context-window.gif)

Давайте пройдем по фиолетовой траектории. Как система обрабатывает слово «robotics» и генерирует «A»?

Высокоуровнево шаги выглядят так:

1.	Преобразование слова в его векторное представление (набор чисел).
2.	Вычисление предсказания.
3.	Преобразование полученных векторов в слово.

![06-gpt3-embedding](https://habrastorage.org/webt/wv/zo/cp/wvzocpynvp5g-quyu7lny9dw6ze.gif)

Важные вычисления GPT-3 происходят внутри стека из 96 слоев декодера Трансформера.

Видите все эти слои? Это и есть та самая «глубина» «глубокого обучения» (deep learning).

У каждого слоя есть свои 1.8 миллиардов параметров для вычислений. Здесь и происходит вся «магия». Верхнеуровнево этот процесс можно изобразить следующим образом:

![07-gpt3-processing-transformer-blocks](https://habrastorage.org/webt/i2/qn/fv/i2qnfvpkrvls2ot_sywwh8l8oou.gif)

Вы можете увидеть детальное описание всего, что происходит внутри декодера, в статье [GTP-2 в картинках](https://habr.com/ru/post/490842/).

Отличие GPT-3 состоит в изменении плотных (dense) и разреженных (sparse) слоев внутреннего внимания (self-attention).

Рассмотрим подробнее пример ввода предложения и вывода ответа «Okay human» внутри GPT-3. Обратите внимание, как каждый токен проходит через все слои стека. Нам не важен выход для первых слов: он начинает иметь значение, только когда ввод окончен. Далее мы отправляем слова выхода обратно в модель.

![08-gpt3-tokens-transformer-blocks](http://jalammar.github.io/images/gpt3/08-gpt3-tokens-transformer-blocks.gif)

В [примере генерации React кода](https://twitter.com/sharifshameem/status/1284421499915403264) на вход подается описание (выделено зеленым), по всей видимости, в дополнение к нескольким примерам вида описание => код. Затем код React генерируется точно так же, как и розовые токены здесь, один за другим.

Можно предположить, что начальные примеры и описания были добавлены на вход модели вместе со специальными токенами, отделяющими примеры от результата.

![09-gpt3-generating-react-code-example](http://jalammar.github.io/images/gpt3/09-gpt3-generating-react-code-example.gif)

То, как это работает, впечатляет. Вам просто нужно подождать, пока завершится тонкая настройка (fine-tuning) GPT-3. И возможности буду еще более потрясающими.

Тонкая настройка просто обновляет веса модели для того, чтобы улучшить ее результат для конкретной задачи.

![10-gpt3-fine-tuning](https://habrastorage.org/webt/jm/tu/be/jmtubeqc2vhq3qj17u6a4qlptcs.gif)

# Авторы

* **Автор оригинала** – [Jay Alammar](http://jalammar.github.io/how-gpt3-works-visualizations-animations/)
* **Перевод** – [Смирнова Екатерина](https://habr.com/ru/users/smekur/)
* **Редактирование и вёрстка** – [Шкарин Сергей](https://habr.com/ru/users/kouki_rus/)