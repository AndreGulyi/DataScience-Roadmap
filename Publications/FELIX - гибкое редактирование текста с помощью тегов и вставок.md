[Seq2seq](https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf) модели набирают все большую популярность для решения различных задач генерации естественного языка (NLG), начиная от [машинного перевода](https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html) и заканчивая одноязычными задачами генерации текста, такими как [суммаризация](https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html), [объединение предложений](https://research.google/tools/datasets/discofuse/), упрощение текста и постредактирование машинного перевода. Однако для многих одноязычных задач эти модели кажутся неоптимальным выбором, поскольку желаемый выходной текст часто представляет собой незначительную переработку входного текста. При выполнении таких задач модели seq2seq работают медленнее, потому что они генерируют вывод по одному слову за раз (т.е. [авторегрессионно](https://en.wikipedia.org/wiki/Autoregressive_model)), и слишком затратны, потому что большинство входных токенов просто копируются в вывод.

В связи с этим большой интерес вызвали [модели редактирования текста](https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html), поскольку они предлагают прогнозировать операции редактирования, такие как удаление, вставка или замена слов, которые применяются к входным данным для получения восстановленных выходных данных. Однако предыдущие подходы к редактированию текста имеют ограничения. Они либо быстрые (не авторегрессионные), но не гибкие, потому что используют ограниченное количество операций редактирования, либо гибкие, поддерживающие все возможные операции редактирования, но медленные (авторегрессионные). В любом случае они не предназначены для моделирования больших структурированных (синтаксических) преобразований, например переключения с действительного залога «Они съели стейк на ужин» на страдательный «Стейк был съеден на ужин». Вместо этого они сосредоточиваются на локальных преобразованиях, удалении или замене коротких фраз. Когда требуется крупное структурное преобразование, они либо не могут его произвести, либо вставляют большой объем нового текста, что существенно замедляет обработку.

В статье «[FELIX: Flexible Text Editing Through Tagging and Insertion](https://www.aclweb.org/anthology/2020.findings-emnlp.111/)» авторы представляют быструю и гибкую систему редактирования текста FELIX, которая моделирует большие структурные преобразования и обеспечивает 90-кратное ускорение по сравнению с подходами seq2seq, при этом достигая впечатляющих результатов в четырех задачах одноязычной генерации. По сравнению с традиционными методами seq2seq, FELIX имеет следующие три ключевых преимущества:

*   *Эффективность выборки*: для обучения модели генерации текста высокой точности обычно требуются большие объемы высококачественных данных. FELIX использует три метода для минимизации количества требуемых данных: (1) тонкая настройка предварительно обученных контрольных точек, (2) модель тегов, которая обучается на небольшом количестве операций редактирования, и (3) задача вставки текста, сходная задаче предварительного обучения модели.
*   *Быстрое время вывода*: FELIX является полностью неавторегрессионным, что позволяет избежать медленной работы, вызванной авторегрессионным декодером.
*   *Гибкое редактирование текста*: FELIX обеспечивает баланс между сложностью выученных операций редактирования и гибкостью моделируемых преобразований.

Таким образом, FELIX предназначен для получения максимальной выгоды от предварительного самообучения, будучи эффективным в условиях ограниченных ресурсов и небольшого количества данных для обучения.

# Обзор

Чтобы достичь всего этого, FELIX разбивает задачу редактирования текста на две подзадачи: тегирование для определения подмножества входных слов и их порядка в выходном тексте и вставка, когда вставляются слова, которых нет во входных данных. В модели тегов используется новый указывающий механизм (a pointer mechanism), который поддерживает структурные преобразования, в то время как модель вставки основана на [маскированной языковой модели](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html) (Masked Language Model, MLM). Обе эти модели не являются авторегрессионными, что обеспечивает высокую производительность. Схему FELIX можно увидеть ниже.

![image2](https://habrastorage.org/webt/jg/zt/hg/jgzthg4bune6zv3tur2fabm9bee.gif)

*Пример обучения FELIX на данных для задачи упрощения текста. Входные слова сначала помечаются как KEEP (K) – «оставить», DELETE (D) – «удалить» или KEEP и INSERT (I) – «оставить и вставить». После добавления тегов порядок ввода изменяется. Этот переупорядоченный ввод затем подается в маскированную языковую модель.*

# Модель тегов

Первым шагом в FELIX является модель тегов, которая состоит из двух компонентов. Сначала теггер определяет, какие слова следует сохранить или удалить и куда следует вставить новые слова. Когда теггер предсказывает вставку, к выходным данным добавляется специальный токен MASK. После тегирования происходит этап переупорядочения, на котором указатель изменяет порядок входной последовательности для формирования выхода, с помощью которого можно будет повторно использовать части этой последовательности вместо вставки нового текста. Этап изменения порядка поддерживает произвольные перезаписи, что позволяет моделировать большие изменения. Сеть указателей обучается таким образом, что каждое слово на входе указывает на следующее слово, как оно будет отображаться на выходе (см. рис).

![image1](https://habrastorage.org/webt/am/dq/5u/amdq5uy19qu08x0f7gmeasrviyu.gif)

*Реализация механизма наведения для преобразования предложения «There are 3 layers in the walls of the heart» в последовательность «the heart MASK 3 layers».*

# Модель вставки

Результатом модели тегирования является переупорядоченный входной текст с удаленными словами и токенами MASK, предсказанными тегом вставки. Модель вставки должна предсказывать содержимое токенов MASK. Поскольку модель вставки FELIX очень похожа на цель предварительного обучения [BERT](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), она может напрямую использовать преимущества предварительного обучения, что особенно полезно, когда данные ограничены.

![image5](https://habrastorage.org/webt/8d/es/v7/8desv7yfd_rxdjsrpnsydshxtqc.png)

*Пример модели вставки, где теггер предсказывает, что будут вставлены два слова, а модель вставки предсказывает содержимое токенов MASK.*

# Полученные результаты

Авторы оценили результаты FELIX на задачах объединения предложений, упрощения текста, абстрактной суммаризации и постредактирования машинного перевода. Эти задачи значительно различаются по типам требуемых изменений и размерам наборов данных, с которыми они работают. Ниже приведены результаты задачи объединения предложений (т.е. соединения двух предложений в одно), сравнение FELIX с большой предварительно обученной моделью seq2seq ([BERT2BERT](https://www.aclweb.org/anthology/2020.tacl-1.18/)) и моделью редактирования текста ([LaserTager](https://ai.googleblog.com/2020/01/encode-tag-and-realize-controllable-and.html)) при различных размерах наборов данных. Как можно заметить, FELIX превосходит LaserTagger и может быть обучен всего на нескольких сотнях обучающих примеров. Для полного набора данных авторегрессионный BERT2BERT превосходит FELIX. Однако время его работы значительно больше времени, за которое отрабатывает FELIX.

![image4](https://habrastorage.org/webt/qp/7c/pn/qp7cpn0apeiuqmrktzwyxt4z3ue.png)

*Сравнение различных размеров обучающих подвыборок набора данных DiscoFuse. Авторы сравнивают FELIX (наиболее эффективную из моделей) с BERT2BERT и LaserTagger.*

![image3](https://habrastorage.org/webt/ah/nl/vq/ahnlvqgz_b_zkoecxbesjcqf5f4.png)

*Задержка в миллисекундах для батча размером 32 на Nvidia Tesla P100.*

# Заключение

Модель FELIX, будучи неавторегрессионной, обеспечивает еще более быстрое время работы при достижении наилучших результатов. FELIX также минимизирует объем необходимых обучающих данных с помощью трех методов - тонкой настройки предварительно обученных контрольных точек, обучения небольшому количеству операций редактирования и задачи вставки, которая имитирует задачу маскированной языковой модели из предварительного обучения. Наконец, FELIX обеспечивает баланс между сложностью выученных операций редактирования и процентом преобразований ввода-вывода, которые он может обработать. Авторы открыли [исходный код](https://github.com/google-research/google-research/tree/master/felix) для FELIX и надеются, что он предоставит исследователям более быструю, более эффективную и гибкую модель редактирования текста.

# Авторы

* **Автор оригинала** – Jonathan Mallinson, Aliaksei Severyn
* **Перевод** – [Смирнова Екатерина](https://habr.com/ru/users/smekur/)
* **Редактирование и вёрстка** – [Шкарин Сергей](https://habr.com/ru/users/kouki_rus/)